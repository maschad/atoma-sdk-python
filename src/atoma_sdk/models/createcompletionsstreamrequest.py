"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionsprompt import CompletionsPrompt, CompletionsPromptTypedDict
from .streamoptions import StreamOptions, StreamOptionsTypedDict
from atoma_sdk.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Dict, List, Optional
from typing_extensions import NotRequired, TypedDict


class CreateCompletionsStreamRequestTypedDict(TypedDict):
    model: str
    r"""ID of the model to use"""
    prompt: CompletionsPromptTypedDict
    best_of: NotRequired[Nullable[int]]
    echo: NotRequired[Nullable[bool]]
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their
    existing frequency in the text so far
    """
    logit_bias: NotRequired[Nullable[Dict[str, float]]]
    r"""Modify the likelihood of specified tokens appearing in the completion.

    Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer)
    to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits
    generated by the model prior to sampling. The exact effect will vary per model, but values
    between -1 and 1 should decrease or increase likelihood of selection; values like -100 or
    100 should result in a ban or exclusive selection of the relevant token.
    """
    logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the chat completion"""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message"""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on
    whether they appear in the text so far
    """
    seed: NotRequired[Nullable[int]]
    r"""If specified, our system will make a best effort to sample deterministically"""
    stop: NotRequired[Nullable[List[str]]]
    r"""Up to 4 sequences where the API will stop generating further tokens"""
    stream: NotRequired[bool]
    r"""Whether to stream back partial progress. Must be true for this request type."""
    stream_options: NotRequired[Nullable[StreamOptionsTypedDict]]
    suffix: NotRequired[Nullable[str]]
    r"""The suffix that comes after a completion of inserted text."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2"""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature"""
    user: NotRequired[Nullable[str]]
    r"""A unique identifier representing your end-user"""


class CreateCompletionsStreamRequest(BaseModel):
    model: str
    r"""ID of the model to use"""

    prompt: CompletionsPrompt

    best_of: OptionalNullable[int] = 1

    echo: OptionalNullable[bool] = False

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their
    existing frequency in the text so far
    """

    logit_bias: OptionalNullable[Dict[str, float]] = UNSET
    r"""Modify the likelihood of specified tokens appearing in the completion.

    Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer)
    to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits
    generated by the model prior to sampling. The exact effect will vary per model, but values
    between -1 and 1 should decrease or increase likelihood of selection; values like -100 or
    100 should result in a ban or exclusive selection of the relevant token.
    """

    logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability."""

    max_tokens: OptionalNullable[int] = 16
    r"""The maximum number of tokens to generate in the chat completion"""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message"""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on
    whether they appear in the text so far
    """

    seed: OptionalNullable[int] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically"""

    stop: OptionalNullable[List[str]] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens"""

    stream: Optional[bool] = True
    r"""Whether to stream back partial progress. Must be true for this request type."""

    stream_options: OptionalNullable[StreamOptions] = UNSET

    suffix: OptionalNullable[str] = UNSET
    r"""The suffix that comes after a completion of inserted text."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2"""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature"""

    user: OptionalNullable[str] = UNSET
    r"""A unique identifier representing your end-user"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "best_of",
            "echo",
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "n",
            "presence_penalty",
            "seed",
            "stop",
            "stream",
            "stream_options",
            "suffix",
            "temperature",
            "top_p",
            "user",
        ]
        nullable_fields = [
            "best_of",
            "echo",
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "max_tokens",
            "n",
            "presence_penalty",
            "seed",
            "stop",
            "stream_options",
            "suffix",
            "temperature",
            "top_p",
            "user",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
