"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .chatcompletionmessage import ChatCompletionMessage, ChatCompletionMessageTypedDict
from .chatcompletiontoolsparam import (
    ChatCompletionToolsParam,
    ChatCompletionToolsParamTypedDict,
)
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .streamoptions import StreamOptions, StreamOptionsTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from atoma_sdk.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class CreateChatCompletionStreamRequestTypedDict(TypedDict):
    r"""Represents the chat completion request.

    This is used to represent the chat completion request in the chat completion request.
    It can be either a chat completion or a chat completion stream.
    """

    messages: List[ChatCompletionMessageTypedDict]
    r"""A list of messages comprising the conversation so far"""
    model: str
    r"""ID of the model to use"""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their
    existing frequency in the text so far
    """
    function_call: NotRequired[Any]
    r"""Controls how the model responds to function calls"""
    functions: NotRequired[Nullable[List[Any]]]
    r"""A list of functions the model may generate JSON inputs for"""
    logit_bias: NotRequired[Nullable[Dict[str, float]]]
    r"""Modify the likelihood of specified tokens appearing in the completion.

    Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer)
    to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits
    generated by the model prior to sampling. The exact effect will vary per model, but values
    between -1 and 1 should decrease or increase likelihood of selection; values like -100 or
    100 should result in a ban or exclusive selection of the relevant token.
    """
    max_completion_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the chat completion"""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the chat completion"""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message"""
    parallel_tool_calls: NotRequired[Nullable[bool]]
    r"""Whether to enable parallel tool calls."""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on
    whether they appear in the text so far
    """
    response_format: NotRequired[Nullable[ResponseFormatTypedDict]]
    seed: NotRequired[Nullable[int]]
    r"""If specified, our system will make a best effort to sample deterministically"""
    service_tier: NotRequired[Nullable[str]]
    r"""Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:

    If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
    If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
    If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
    When not set, the default behavior is 'auto'.
    """
    stop: NotRequired[Nullable[List[str]]]
    r"""Up to 4 sequences where the API will stop generating further tokens"""
    stream: NotRequired[bool]
    r"""Whether to stream back partial progress. Must be true for this request type."""
    stream_options: NotRequired[Nullable[StreamOptionsTypedDict]]
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2"""
    tool_choice: NotRequired[Nullable[ToolChoiceTypedDict]]
    tools: NotRequired[Nullable[List[ChatCompletionToolsParamTypedDict]]]
    r"""A list of tools the model may call"""
    top_logprobs: NotRequired[Nullable[int]]
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.
    logprobs must be set to true if this parameter is used.
    """
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature"""
    user: NotRequired[Nullable[str]]
    r"""A unique identifier representing your end-user"""


class CreateChatCompletionStreamRequest(BaseModel):
    r"""Represents the chat completion request.

    This is used to represent the chat completion request in the chat completion request.
    It can be either a chat completion or a chat completion stream.
    """

    messages: List[ChatCompletionMessage]
    r"""A list of messages comprising the conversation so far"""

    model: str
    r"""ID of the model to use"""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their
    existing frequency in the text so far
    """

    function_call: Optional[Any] = None
    r"""Controls how the model responds to function calls"""

    functions: OptionalNullable[List[Any]] = UNSET
    r"""A list of functions the model may generate JSON inputs for"""

    logit_bias: OptionalNullable[Dict[str, float]] = UNSET
    r"""Modify the likelihood of specified tokens appearing in the completion.

    Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer)
    to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits
    generated by the model prior to sampling. The exact effect will vary per model, but values
    between -1 and 1 should decrease or increase likelihood of selection; values like -100 or
    100 should result in a ban or exclusive selection of the relevant token.
    """

    max_completion_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the chat completion"""

    max_tokens: Annotated[
        OptionalNullable[int],
        pydantic.Field(
            deprecated="warning: ** DEPRECATED ** - This will be removed in a future release, please migrate away from it as soon as possible."
        ),
    ] = UNSET
    r"""The maximum number of tokens to generate in the chat completion"""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message"""

    parallel_tool_calls: OptionalNullable[bool] = UNSET
    r"""Whether to enable parallel tool calls."""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on
    whether they appear in the text so far
    """

    response_format: OptionalNullable[ResponseFormat] = UNSET

    seed: OptionalNullable[int] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically"""

    service_tier: OptionalNullable[str] = UNSET
    r"""Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:

    If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
    If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
    If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
    When not set, the default behavior is 'auto'.
    """

    stop: OptionalNullable[List[str]] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens"""

    stream: Optional[bool] = True
    r"""Whether to stream back partial progress. Must be true for this request type."""

    stream_options: OptionalNullable[StreamOptions] = UNSET

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2"""

    tool_choice: OptionalNullable[ToolChoice] = UNSET

    tools: OptionalNullable[List[ChatCompletionToolsParam]] = UNSET
    r"""A list of tools the model may call"""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.
    logprobs must be set to true if this parameter is used.
    """

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature"""

    user: OptionalNullable[str] = UNSET
    r"""A unique identifier representing your end-user"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "frequency_penalty",
            "function_call",
            "functions",
            "logit_bias",
            "max_completion_tokens",
            "max_tokens",
            "n",
            "parallel_tool_calls",
            "presence_penalty",
            "response_format",
            "seed",
            "service_tier",
            "stop",
            "stream",
            "stream_options",
            "temperature",
            "tool_choice",
            "tools",
            "top_logprobs",
            "top_p",
            "user",
        ]
        nullable_fields = [
            "frequency_penalty",
            "functions",
            "logit_bias",
            "max_completion_tokens",
            "max_tokens",
            "n",
            "parallel_tool_calls",
            "presence_penalty",
            "response_format",
            "seed",
            "service_tier",
            "stop",
            "stream_options",
            "temperature",
            "tool_choice",
            "tools",
            "top_logprobs",
            "top_p",
            "user",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
